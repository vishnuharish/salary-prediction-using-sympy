{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a2a037b",
   "metadata": {},
   "source": [
    "# Simple Linear Regression for Salary Prediction\n",
    "\n",
    "This notebook provides a step-by-step mathematical representation of the univariate linear regression model using SymPy, applied to predict salaries based on a dataset. We will derive the model equations, explore data preprocessing, model training, evaluation, and visualization to understand the relationship between the predictor variable (e.g., years of experience) and the target variable (salary). The goal is to build a predictive model that can estimate salaries accurately while illustrating the underlying mathematics.\n",
    "\n",
    "## Tools Used\n",
    "- **Python**: Programming language for data analysis and machine learning.\n",
    "- **Pandas**: For data manipulation and analysis.\n",
    "- **NumPy**: For numerical computations.\n",
    "- **SymPy**: For symbolic mathematics and step-by-step mathematical derivations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9597b8a6",
   "metadata": {},
   "source": [
    "## Linear Regression Formula\n",
    "\n",
    "Linear regression finds the best straight line that fits through our data. The line can be described with a simple equation:\n",
    "\n",
    "\n",
    "| *y = mx + c* |\n",
    "----------------                                                                        \n",
    "\n",
    "Where:\n",
    "- **y** = The value we want to predict (salary)\n",
    "- **x** = The input variable (years of experience)\n",
    "- **m** = The slope (how much y changes when x increases by 1)\n",
    "- **c** = The intercept (where the line crosses the y-axis)\n",
    "\n",
    "The model works by finding the values of m and b that make the line fit the data as closely as possible. We do this by minimizing the total squared distance between the actual data points and the predicted line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87f1e6c",
   "metadata": {},
   "source": [
    "## Cost Function (Mean Squared Error)\n",
    "\n",
    "The cost function measures how far our predictions are from the actual values. It's the function we want to minimize to find the best line.\n",
    "\n",
    "### Formula\n",
    "\n",
    "$$J(m, c) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_{pred} - y_{actual})^2$$\n",
    "\n",
    "**Expanded form:**\n",
    "\n",
    "$$J(m, c) = \\frac{1}{2n} \\sum_{i=1}^{n}(mx + c - y)^2$$\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **m** : Slope of the line (weight) - determines the steepness\n",
    "- **c** : Y-intercept (bias) - where the line crosses the y-axis\n",
    "- **x** : Input variable (years of experience)\n",
    "- **y** : Actual output value (salary)\n",
    "- **n** : Total number of data points\n",
    "- **y_pred = mx + c** : Predicted value for a given x\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Calculate errors**: For each data point, find the difference between the predicted value (mx + c) and the actual value (y)\n",
    "2. **Square the errors**: Square each difference to penalize larger errors more heavily and make all errors positive\n",
    "3. **Sum squared errors**: Add up all the squared differences\n",
    "4. **Average the errors**: Divide by 2n to normalize the cost (the factor of 2 is for mathematical convenience in derivatives)\n",
    "5. **Minimize**: Find the values of m and c that minimize J(m, c) to get the best-fitting line\n",
    "\n",
    "The division by 2n ensures that the cost doesn't grow with the size of the dataset, making it comparable across different datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd3887f",
   "metadata": {},
   "source": [
    "## Implementation of Linear Regression using Symbolic Mathematics\n",
    "\n",
    "In this section, we will implement the linear regression model using SymPy's symbolic mathematics capabilities. This approach allows us to derive the exact mathematical solution by computing partial derivatives and solving a system of equations analytically. Unlike iterative methods like gradient descent, this method provides the closed-form solution directly.\n",
    "\n",
    "### Step 1: Import Required Libraries\n",
    "\n",
    "We start by importing the necessary libraries for our implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "89434500",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sympy as sym\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e9b7b1",
   "metadata": {},
   "source": [
    "### Step 2: Load and Prepare the Dataset\n",
    "\n",
    "We load the salary dataset and prepare it for training and testing. The dataset contains years of experience and corresponding salaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c104e76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/Salary_Data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a16102",
   "metadata": {},
   "source": [
    "### Step 3: Split Data into Training Set\n",
    "\n",
    "We extract the first 28 samples as training data. X contains the independent variable (years of experience) and y contains the dependent variable (salary).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50cc275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[0:28, 0].values\n",
    "y = df.iloc[0:28, 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665894ee",
   "metadata": {},
   "source": [
    "### Step 4: Define Symbolic Variables\n",
    "\n",
    "We define the symbolic variables that represent the parameters of our linear regression model. Here, c is the intercept and m is the slope.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a051c0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c, m = sym.symbols(\"c m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca37f9e",
   "metadata": {},
   "source": [
    "### Step 5: Calculate Sum of Squared Errors (SSE)\n",
    "\n",
    "We compute the Sum of Squared Errors (SSE) for all training data points. This represents the total error between predicted and actual values. SSE is the cost function we want to minimize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9d4bd8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = 0\n",
    "for x_i, y_i in zip(X, y):\n",
    "  prediction = m * x_i + c\n",
    "  sse += (y_i - prediction) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571dfab",
   "metadata": {},
   "source": [
    "### Step 6: Compute Partial Derivatives\n",
    "\n",
    "We compute the partial derivatives of SSE with respect to m and c. These derivatives represent the gradients that tell us how to adjust the parameters to minimize the error.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ddc8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff_m = sse.diff(m)\n",
    "diff_c = sse.diff(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ec2ba7",
   "metadata": {},
   "source": [
    "### Step 7: Solve for Optimal Parameters\n",
    "\n",
    "We solve the system of equations (setting both partial derivatives to zero) to find the optimal values of m and c. This is the closed-form solution that minimizes the SSE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "17db9733",
   "metadata": {},
   "outputs": [],
   "source": [
    "solution = sym.solve([diff_m, diff_c], (m,c))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d4272b",
   "metadata": {},
   "source": [
    "### Step 8: Extract Model Parameters\n",
    "\n",
    "We extract the computed slope (m) and intercept (c) values from the solution. These are the parameters of our trained linear regression model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89c14e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "intercept = solution[c]\n",
    "slope = solution[m]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9488d057",
   "metadata": {},
   "source": [
    "### Step 9: Display the Trained Model Equation\n",
    "\n",
    "We display the final linear regression equation with the computed slope and intercept values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edde0068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model: y = 9570.07159301477x + 25336.2527574340\n"
     ]
    }
   ],
   "source": [
    "print(f\" Model: y = {slope}x + {intercept}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ad977f",
   "metadata": {},
   "source": [
    "### Step 10: Create Prediction Function\n",
    "\n",
    "We define a function that uses the trained model to make predictions on new input data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a62c3e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_test):\n",
    "    return float(slope * x_test + intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901c60f8",
   "metadata": {},
   "source": [
    "### Step 11: Prepare Test Data - Extracts test samples\n",
    "\n",
    "We extract the remaining samples (from index 28 onwards) as test data to evaluate the performance of our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d00a9db",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df.iloc[28: , 0].values\n",
    "y_test = df.iloc[28: , 1].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28fd5b84",
   "metadata": {},
   "source": [
    "### Step 12: Make Predictions on Test Data - Tests the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2310846",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125822.00448408909"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = predict(X_test[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e5a146",
   "metadata": {},
   "source": [
    "### Step 13: Compare Predicted vs Actual Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "460d6bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125822.00448408909"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bec28d23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(121872.0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple-linear-regression-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
