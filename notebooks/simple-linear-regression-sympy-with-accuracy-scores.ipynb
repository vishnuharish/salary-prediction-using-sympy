{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4319515a",
   "metadata": {
    "id": "4319515a"
   },
   "source": [
    "# Simple Linear Regression for Salary Prediction\n",
    "\n",
    "This notebook provides a step-by-step mathematical representation of the univariate linear regression model using SymPy, applied to predict salaries based on a dataset. We will derive the model equations, explore data preprocessing, model training, evaluation, and visualization to understand the relationship between the predictor variable (e.g., years of experience) and the target variable (salary). The goal is to build a predictive model that can estimate salaries while illustrating the underlying mathematics.\n",
    "\n",
    "## Steps to derive\n",
    "1. Define Symbolic Variables.\n",
    "2. Defining HYPOTHESIS in symbolic notations.\n",
    "3. Defining COST FUNCTION Mean Squared Error (MSE).\n",
    "4. DIFFERENTIAL EQUATIONS & GRADIENTS.\n",
    "5. DATA LOADING WITH PANDAS.\n",
    "6. GRADIENT DESCENT TRAINING.\n",
    "7. ACCURACY & VISUALIZATION.\n",
    "8. COMPUTING THE TOTAL COST.\n",
    "9. CONTOUR PLOT VISUALIZATION.\n",
    "\n",
    "## Tools Used\n",
    "- **Python**: Programming language for data analysis and machine learning.\n",
    "- **Pandas**: For data manipulation and analysis.\n",
    "- **NumPy**: For numerical computations.\n",
    "- **SymPy**: For symbolic mathematics and step-by-step mathematical derivations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4baa49b5",
   "metadata": {
    "id": "4baa49b5"
   },
   "source": [
    "## Linear Regression Formula\n",
    "\n",
    "Linear regression finds the best straight line that fits through our data. The line can be described with a simple equation:\n",
    "\n",
    "***$$h = mx + b$$***\n",
    "\n",
    "Where:\n",
    "- **h** = The value we want to predict (salary)\n",
    "- **x** = The input variable (years of experience)\n",
    "- **m** = The slope (how much y changes when x increases by 1)\n",
    "- **b** = The intercept (where the line crosses the y-axis)\n",
    "\n",
    "The model works by finding the values of m and b that make the line fit the data as closely as possible. We do this by minimizing the total squared distance between the actual data points and the predicted line.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afb1537f",
   "metadata": {
    "id": "afb1537f"
   },
   "source": [
    "## Cost Function (Mean Squared Error)\n",
    "\n",
    "The cost function measures how far our predictions are from the actual values. It's the function we want to minimize to find the best line.\n",
    "\n",
    "### Formula\n",
    "\n",
    "***$$J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n}(y_{pred} - y_{actual})^2$$***\n",
    "\n",
    "**Expanded form:**\n",
    "\n",
    "***$$J(m, b) = \\frac{1}{2n} \\sum_{i=1}^{n}(mx + b - y)^2$$***\n",
    "\n",
    "### Parameters\n",
    "\n",
    "- **m** : Slope of the line (weight) - determines the steepness\n",
    "- **b** : Y-intercept (bias) - where the line crosses the y-axis\n",
    "- **x** : Input variable (years of experience)\n",
    "- **y** : Actual output value (salary)\n",
    "- **n** : Total number of data points\n",
    "- **y_pred = mx + c** : Predicted value for a given x\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. **Calculate errors**: For each data point, find the difference between the predicted value (mx + c) and the actual value (y)\n",
    "2. **Square the errors**: Square each difference to penalize larger errors more heavily and make all errors positive\n",
    "3. **Sum squared errors**: Add up all the squared differences\n",
    "4. **Average the errors**: Divide by 2n to normalize the cost (the factor of 2 is for mathematical convenience in derivatives)\n",
    "5. **Minimize**: Find the values of m and c that minimize J(m, c) to get the best-fitting line\n",
    "\n",
    "The division by 2n ensures that the cost doesn't grow with the size of the dataset, making it comparable across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1ddf90d3",
   "metadata": {
    "id": "1ddf90d3"
   },
   "outputs": [],
   "source": [
    "import sympy as sp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9571b045",
   "metadata": {
    "id": "9571b045"
   },
   "source": [
    "## STEP 1: Define Symbolic Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "4fb4ce59",
   "metadata": {
    "id": "4fb4ce59"
   },
   "outputs": [],
   "source": [
    "\n",
    "sp.init_printing(use_unicode=False, use_latex=\"mathjax\")\n",
    "m, b, x, y = sp.symbols('m b x y')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c937b534",
   "metadata": {
    "id": "c937b534"
   },
   "source": [
    "## STEP 2: Defining HYPOTHESIS in symbolic notations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "e224ca93",
   "metadata": {
    "id": "e224ca93",
    "outputId": "78232bdb-8d9e-49c5-d4d4-24806b62fe9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Hypothesis Formula\n",
      "b + m*x\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hypothesis = m * x + b\n",
    "print(\"\\nStep 2: Hypothesis Formula\")\n",
    "sp.pprint(hypothesis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55015eea",
   "metadata": {
    "id": "55015eea"
   },
   "source": [
    "## STEP 3: Defining COST FUNCTION Mean Squared Error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "d36016cc",
   "metadata": {
    "id": "d36016cc",
    "outputId": "38ccb477-5159-4782-d25e-13a7141d2f9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Cost Function (Single Point MSE)\n",
      "             2\n",
      "(b + m*x - y) \n"
     ]
    }
   ],
   "source": [
    "\n",
    "cost_func = ((hypothesis) - y)**2\n",
    "print(\"\\nStep 3: Cost Function (Single Point MSE)\")\n",
    "sp.pprint(cost_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d90e1",
   "metadata": {
    "id": "f09d90e1"
   },
   "source": [
    "## STEP 4: DIFFERENTIAL EQUATIONS & GRADIENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "2ca59e97",
   "metadata": {
    "id": "2ca59e97"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create unevaluated derivative objects for display\n",
    "grad_m_eqn = sp.Derivative(cost_func, m)\n",
    "grad_b_eqn = sp.Derivative(cost_func, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "c3ea4616",
   "metadata": {
    "id": "c3ea4616"
   },
   "outputs": [],
   "source": [
    "# Calculate the actual derivative expressions\n",
    "grad_m_expr = grad_m_eqn.doit()\n",
    "grad_b_expr = grad_b_eqn.doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "cb75d77f",
   "metadata": {
    "id": "cb75d77f",
    "outputId": "23c203e7-a48d-4a69-ed29-b7f0ea862433"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4: Symbolic Gradients (Calculus)\n",
      "Partial Derivative for Weight (m):\n",
      "\n",
      "\n",
      "\n",
      "d /             2\\\n",
      "--\\(b + m*x - y) /\n",
      "dm                \n",
      "Evaluates to:\n",
      "2*x*(b + m*x - y)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4: Symbolic Gradients (Calculus)\", end=\"\\n\")\n",
    "print(\"Partial Derivative for Weight (m):\", end=\"\\n\")\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "sp.pprint(grad_m_eqn)\n",
    "print(\"Evaluates to:\")\n",
    "sp.pprint(grad_m_expr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "c393f4d3",
   "metadata": {
    "id": "c393f4d3",
    "outputId": "1f6416eb-dcf8-4e6f-d459-7b2d79084228"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partial Derivative for Bias (b):\n",
      "\n",
      "\n",
      "\n",
      "d /             2\\\n",
      "--\\(b + m*x - y) /\n",
      "db                \n",
      "Evaluates to:\n",
      "2*b + 2*m*x - 2*y\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nPartial Derivative for Bias (b):\")\n",
    "\n",
    "\n",
    "print(\"\\n\\n\")\n",
    "sp.pprint(grad_b_eqn)\n",
    "print(\"Evaluates to:\")\n",
    "sp.pprint(grad_b_expr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4985d",
   "metadata": {
    "id": "bcd4985d"
   },
   "source": [
    "## STEP 5: DATA LOADING WITH PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "6d7ebb69",
   "metadata": {
    "id": "6d7ebb69",
    "outputId": "25298769-2e77-493c-c879-c3f6ca8dcbf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Loading Data with Pandas\n",
      "\n",
      "Data successfully loaded from CSV using Pandas.\n",
      "X_train =  [ 1.1  1.3  1.5  2.   2.2  2.9  3.   3.2  3.2  3.7  3.9  4.   4.   4.1\n",
      "  4.5  4.9  5.1  5.3  5.9  6.   6.8  7.1  7.9  8.2  8.7  9.   9.5  9.6\n",
      " 10.3 10.5]\n",
      "Y_train =  [ 39343.  46205.  37731.  43525.  39891.  56642.  60150.  54445.  64445.\n",
      "  57189.  63218.  55794.  56957.  57081.  61111.  67938.  66029.  83088.\n",
      "  81363.  93940.  91738.  98273. 101302. 113812. 109431. 105582. 116969.\n",
      " 112635. 122391. 121872.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nStep 5: Loading Data with Pandas\", end=\"\\n\")\n",
    "try:\n",
    "    # Replace 'data.csv' with your actual file path\n",
    "    df = pd.read_csv('../data/Salary_Data.csv')\n",
    "    X_train = df['YearsExperience'].values\n",
    "    Y_train = df['Salary'].values\n",
    "    print(\"\\nData successfully loaded from CSV using Pandas.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"\\nCSV not found. Using placeholder data for demonstration.\")\n",
    "    X_train = np.array([1, 2, 3, 4, 5])\n",
    "    Y_train = np.array([3.1, 4.9, 7.2, 8.8, 11.1])\n",
    "\n",
    "print(\"X_train = \" ,X_train)\n",
    "print(\"Y_train = \", Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfca915d",
   "metadata": {
    "id": "bfca915d"
   },
   "source": [
    "## STEP 6: GRADIENT DESCENT TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bdecb6d4",
   "metadata": {
    "id": "bdecb6d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_linear_regression(data_x, data_y, lr=0.01, epochs=1000):\n",
    "    curr_m, curr_b = 0.0, 0.0\n",
    "    n = len(data_x)\n",
    "    print(f\"\\nStep 6: Training (LR={lr}, Epochs={epochs})...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        sum_grad_m = 0\n",
    "        sum_grad_b = 0\n",
    "        for xi, yi in zip(data_x, data_y):\n",
    "            subs = {m: curr_m, b: curr_b, x: xi, y: yi}\n",
    "            sum_grad_m += grad_m_expr.subs(subs)\n",
    "            sum_grad_b += grad_b_expr.subs(subs)\n",
    "\n",
    "        curr_m -= lr * (sum_grad_m / n)\n",
    "        curr_b -= lr * (sum_grad_b / n)\n",
    "\n",
    "        if epoch % 250 == 0:\n",
    "            print(f\"  Epoch {epoch}: m = {float(curr_m):.4f}, b = {float(curr_b):.4f}\")\n",
    "\n",
    "    return float(curr_m), float(curr_b)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e90212f",
   "metadata": {
    "id": "0e90212f"
   },
   "source": [
    "## STEP 7: ACCURACY & VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "cdbcb124",
   "metadata": {
    "id": "cdbcb124"
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_and_plot(data_x, data_y, fm, fb):\n",
    "    y_true = np.array(data_y)\n",
    "    y_pred = fm * data_x + fb\n",
    "\n",
    "    # Accuracy Metrics\n",
    "    r2 = 1 - (np.sum((y_true - y_pred)**2) / np.sum((y_true - np.mean(y_true))**2))\n",
    "    mae = np.mean(np.abs(y_true - y_pred))\n",
    "\n",
    "    print(\"\\nStep 7: Accuracy Metrics\")\n",
    "    print(f\"  - RÂ² Score: {r2:.4f}\")\n",
    "    print(f\"  - Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.scatter(data_x, data_y, color='red', label='Actual Data')\n",
    "    plt.plot(data_x, y_pred, color='blue', label=f'Model: y={fm:.2f}x + {fb:.2f}')\n",
    "    plt.title('Linear Regression Fit (SymPy + Pandas)')\n",
    "    plt.xlabel('X')\n",
    "    plt.ylabel('Y')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab4b24",
   "metadata": {
    "id": "89ab4b24"
   },
   "source": [
    "## STEP 8: COMPUTING THE TOTAL COST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "65be45db",
   "metadata": {
    "id": "65be45db"
   },
   "outputs": [],
   "source": [
    "cost_lambda = sp.lambdify((m, b, x, y), cost_func, 'numpy')\n",
    "\n",
    "def compute_total_cost(m_val, b_val, x_data, y_data):\n",
    "    # Calculates the average MSE for a grid of m and b values\n",
    "    total_error = 0\n",
    "    for xi, yi in zip(x_data, y_data):\n",
    "        total_error += cost_lambda(m_val, b_val, xi, yi)\n",
    "    return total_error / len(x_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b3628",
   "metadata": {
    "id": "e18b3628"
   },
   "source": [
    "## STEP 7: CONTOUR PLOT VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd17b1",
   "metadata": {
    "id": "a9cd17b1",
    "outputId": "11bde8af-42a8-405f-a29b-521e05d7f6a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 6: Training (LR=0.01, Epochs=1000)...\n",
      "  Epoch 0: m = 9547.9740, b = 1520.0600\n",
      "  Epoch 250: m = 10671.8689, b = 17558.0961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def plot_cost_contour(data_x, data_y, fm, fb):\n",
    "    # 1. Create a grid of m and b values around our final solution\n",
    "    m_range = np.linspace(fm - 2, fm + 2, 50)\n",
    "    b_range = np.linspace(fb - 2, fb + 2, 50)\n",
    "    M, B = np.meshgrid(m_range, b_range)\n",
    "\n",
    "    # 2. Compute cost for every point on the grid\n",
    "    Z = np.array([compute_total_cost(mv, bv, data_x, data_y) for mv, bv in zip(np.ravel(M), np.ravel(B))])\n",
    "    Z = Z.reshape(M.shape)\n",
    "\n",
    "    # 3. Plotting\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    cp = plt.contourf(M, B, Z, levels=20, cmap='viridis') # Filled contours\n",
    "    plt.colorbar(cp, label='Cost (MSE)')\n",
    "\n",
    "    # Mark the final optimized point\n",
    "    plt.plot(fm, fb, 'ro', label=f'Minimum (m={fm:.2f}, b={fb:.2f})')\n",
    "\n",
    "    plt.title('Cost Function Contour Landscape')\n",
    "    plt.xlabel('Slope (m)')\n",
    "    plt.ylabel('Intercept (b)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "final_m, final_b = train_linear_regression(X_train, Y_train)\n",
    "evaluate_and_plot(X_train, Y_train, final_m, final_b)\n",
    "plot_cost_contour(X_train, Y_train, final_m, final_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8344acc9",
   "metadata": {
    "id": "8344acc9"
   },
   "outputs": [],
   "source": [
    "# # Testing the Model\n",
    "# df_test = pd.read_csv(\"../data/Salary_Data_test.csv\")\n",
    "\n",
    "# df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f9e3dd",
   "metadata": {
    "id": "86f9e3dd"
   },
   "outputs": [],
   "source": [
    "# X_test = df_test[\"YearsExperience\"].values\n",
    "# y_test = df_test[\"Salary\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af5c0a4",
   "metadata": {
    "id": "6af5c0a4"
   },
   "outputs": [],
   "source": [
    "# X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81e3b21",
   "metadata": {
    "id": "d81e3b21"
   },
   "outputs": [],
   "source": [
    "# y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c53cfe2",
   "metadata": {
    "id": "8c53cfe2"
   },
   "outputs": [],
   "source": [
    "# def predict(x_test):\n",
    "#     y_preds = []\n",
    "#     for x_t in x_test:\n",
    "#         res = final_m * x_t + final_b\n",
    "#         y_preds.append(res)\n",
    "#     return y_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31755a2a",
   "metadata": {
    "id": "31755a2a"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
